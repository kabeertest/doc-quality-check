# Document Quality Checker - National ID Detection System

A production-ready Streamlit application for validating document quality and automatically detecting identity cards with advanced classification and visualization.

---

## ğŸ“‹ Table of Contents

1. [Features](#features)
2. [Quick Start](#quick-start)
3. [Project Structure](#project-structure)
4. [Installation](#installation)
5. [Usage](#usage)
6. [Configuration](#configuration)
7. [Detection System](#detection-system)
8. [Technical Architecture](#technical-architecture)

---

## ğŸ¯ Features

### Document Quality Validation
- **Emptiness Detection**: Identifies blank or near-blank pages
- **Readability Analysis**: Validates OCR confidence scores
- **Clarity Assessment**: Measures ink ratio and document content density
- **Quality Metrics**: Per-page reporting with configurable thresholds

### Identity Card Detection & Classification
- **Multi-Document Support**: Detects and processes multiple documents on single pages
- **Document Type Classification**: National ID, Passport, Driver's License, etc.
- **Side Detection**: Automatically classifies front and back using MRZ pattern and keywords
- **Confidence Scoring**: 0-100% confidence for each classification with detailed breakdown
- **Bounding Box Visualization**: Interactive marked document locations with color-coded boxes
- **MRZ Pattern Detection**: Reliable back-side detection using Machine Readable Zone
- **Intelligent Pairing**: Automatically pairs front/back documents on the same page
- **Content-Based Detection**: Uses actual document content, not position

### Advanced Features
- **Text Cleaning**: Removes OCR artifacts (????, control chars, null bytes)
- **Adaptive OCR**: Switches between fast and full modes based on quality
- **Keyword Frequency Analysis**: Boosts confidence when keywords appear across documents
- **Heuristic Matching**: Fallback logic for ambiguous documents
- **Comprehensive Logging**: Detailed tracking of detection methods and confidence adjustments

---

## ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Install Tesseract OCR (Windows)
# Download from: https://github.com/UB-Mannheim/tesseract/wiki
# On macOS: brew install tesseract
# On Linux: sudo apt-get install tesseract-ocr

# 3. Run application
streamlit run app.py

# 4. Open browser to http://localhost:8501
```

---

## ğŸ“ Project Structure

```
doc-quality-check/
â”œâ”€â”€ ğŸ“„ Core Files
â”‚   â”œâ”€â”€ app.py                          # Main Streamlit application
â”‚   â”œâ”€â”€ test_readability.py             # CLI readability test utility
â”‚   â”œâ”€â”€ config.json                     # Document types, keywords, and settings
â”‚   â”œâ”€â”€ requirements.txt                # Python package dependencies
â”‚   â””â”€â”€ README.md                       # Main documentation
â”‚
â”œâ”€â”€ ğŸ“š Documentation (docs/)
â”‚   â”œâ”€â”€ README.md                       # Documentation index
â”‚   â”œâ”€â”€ LANGUAGE_CONFIGURATION_GUIDE.md # Language setup guide
â”‚   â”œâ”€â”€ THRESHOLD_ANALYSIS_REPORT.md    # Threshold analysis
â”‚   â”œâ”€â”€ IMPROVEMENTS_SUMMARY.md         # Recent improvements
â”‚   â”œâ”€â”€ ARTIFACT_FILTERING.md           # Artifact filtering feature
â”‚   â”œâ”€â”€ FULL_TEXT_FEATURE.md            # Full text extraction
â”‚   â””â”€â”€ ITALIAN_ID_ISSUE_ANALYSIS.md    # Italian ID analysis
â”‚
â”œâ”€â”€ ğŸ§ª Tests (tests/)
â”‚   â”œâ”€â”€ README.md                       # Tests index
â”‚   â”œâ”€â”€ analyze_thresholds.py           # Threshold analysis tests
â”‚   â”œâ”€â”€ check_lang.py                   # Language detection tests
â”‚   â”œâ”€â”€ test_filter_comparison.py       # Artifact filtering comparison
â”‚   â”œâ”€â”€ test_improved_confidence.py     # Confidence calculation tests
â”‚   â””â”€â”€ test_italian_summary.py         # Italian ID tests
â”‚
â”œâ”€â”€ ğŸ”§ Modules
â”‚   â”œâ”€â”€ modules/                        # Core detection modules
â”‚   â”‚   â”œâ”€â”€ config_loader.py           # Configuration management
â”‚   â”‚   â”œâ”€â”€ identity_detection.py      # Classification engine
â”‚   â”‚   â”œâ”€â”€ document_segmentation.py   # Multi-document segmentation
â”‚   â”‚   â””â”€â”€ visualization.py           # Bounding box visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                          # Utility modules
â”‚   â”‚   â”œâ”€â”€ document_processor.py      # PDF page extraction
â”‚   â”‚   â”œâ”€â”€ content_extraction.py      # OCR text extraction
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py            # Text cleaning
â”‚   â”‚   â”œâ”€â”€ text_filter.py             # Artifact filtering âœ¨
â”‚   â”‚   â””â”€â”€ logger.py                  # Logging configuration
â”‚   â”‚
â”‚   â””â”€â”€ checks/                         # Quality assessment
â”‚       â”œâ”€â”€ clarity_check.py           # Document clarity analysis
â”‚       â”œâ”€â”€ confidence_check.py        # OCR confidence scoring
â”‚       â””â”€â”€ confidence_check_improved.py # Enhanced confidence âœ¨
â”‚
â”œâ”€â”€ ğŸ“Š Test Data
â”‚   â”œâ”€â”€ dataset/                        # Current test documents
â”‚   â””â”€â”€ dataset-v1/                     # Previous version documents
â”‚
â”œâ”€â”€ ğŸ§ª Experiments (experiments/)
â”‚   â”œâ”€â”€ README.md                       # Experiments index
â”‚   â””â”€â”€ confidence_check_improved.py    # Enhanced OCR (experimental) âœ¨
â”‚
â”œâ”€â”€ ğŸ—‘ï¸ Temporary (temp_debugs/)
â”‚   â””â”€â”€ README.md                       # Temporary files (can delete)
```

---

## ğŸ”§ Installation

### Prerequisites
- Python 3.8+
- Tesseract OCR engine
- 50MB+ disk space

### Step-by-Step

**1. Install Python Dependencies**
```bash
pip install -r requirements.txt
```

**2. Install Tesseract OCR**

Windows:
- Download: https://github.com/UB-Mannheim/tesseract/wiki
- Run installer (default: C:\Program Files\Tesseract-OCR)
- App auto-detects installation

macOS:
```bash
brew install tesseract
```

Linux:
```bash
sudo apt-get install tesseract-ocr
```

**3. Run Application**
```bash
streamlit run app.py
```

---

## ğŸ“– Usage Guide

### User Interface Sections

#### 1. Document Upload & Settings
- Upload PDF or image file
- Configure three key thresholds:
  - **Emptiness Threshold** (1-10%): Minimum page content
  - **Readability Threshold** (0-100%): Minimum OCR confidence
  - **Identity Confidence Threshold** (0-100%, default 70%): Minimum detection confidence

#### 2. Detection Summary
- Page-by-page results in table format
- "National ID Present" shows âœ… Yes or âŒ No based on threshold
- Color-coded rows for quick status assessment

#### 3. Page Visualizations
- Full-page images with bounding boxes
- Segmented documents with colored boxes (one color per document type)
- Card-style display for each detected segment
- Confidence indicators (green/blue/orange)

#### 4. Advanced Analysis (Expandable)
- Segmented document images
- Confidence breakdown with adjustments
- Matched keywords with frequency
- OCR extracted text

### Typical Workflow

```
1. Upload PDF â†’ 2. Set confidence threshold â†’ 3. Review summary table
                    â†“
            4. Check page visualizations
                    â†“
            5. Review advanced analysis
                    â†“
            6. Export or archive results
```

---

## âš™ï¸ Configuration

### config.json - Single Source of Truth

**All configuration is centralized** - no hardcoded values in Python code.

#### Document Types
```json
{
  "document_types": {
    "residential_id": {
      "name": "National ID",
      "display_name": "National ID",
      "enabled": true,
      "label": "ID",
      "color": [255, 0, 0],
      "keywords": {
        "en": ["national id", "identity card"],
        "it": ["carta d'identitÃ ", "id"]
      }
    }
  }
}
```

#### Document Sides
```json
{
  "document_sides": {
    "front": {
      "name": "Front",
      "keywords": {
        "en": ["photo", "name", "date of birth"],
        "it": ["foto", "nome", "data di nascita"]
      }
    },
    "back": {
      "name": "Back",
      "keywords": {
        "en": ["signature", "expiry", "issued by"],
        "it": ["firma", "scadenza", "rilascio"]
      }
    }
  }
}
```

### Customization

**Add New Document Type:**
1. Edit config.json - add entry under `document_types`
2. Define keywords for detection
3. Set RGB color for visualization
4. Reload application

**Modify Keywords:**
Edit keyword lists in config.json and reload

**Adjust Thresholds:**
Use sidebar sliders (no code changes needed)

---

## ğŸ” Detection System

### How It Works

#### Step 1: Document Segmentation
```
Input: PDF page
  â†“
Contour detection (OpenCV)
  â†“
Overlap removal (IoU-based)
  â†“
Projection-based splitting (for side-by-side docs)
  â†“
Output: Individual document segments
```

#### Step 2: Text Extraction (OCR)
```
Input: Document segment
  â†“
Fast mode (PSM 6) - quick extraction
  â†“
Quality check - if <30 chars, retry with full mode
  â†“
Text cleaning - remove artifacts
  â†“
Output: Cleaned text
```

#### Step 3: Classification
```
Input: Cleaned text
  â†“
MRZ Detection (35+ '<' characters) â†’ BACK (strongest indicator)
  â†“
Back Keyword Matching â†’ BACK
  â†“
Front Keyword Matching â†’ FRONT
  â†“
Heuristic Pairing (if ambiguous) â†’ Use other document on page
  â†“
Output: Document type + side + confidence
```

### Confidence Scoring Breakdown

**Base Confidence** (0-100%)
- Front keywords match: 60-100%
- Back keywords match: 60-100%
- MRZ pattern detected: 85-100%

**Adjustments** (can add 0-30%)
- Frequency Boost: Keywords in 2+ documents = +10%, 3+ = +20%
- Specificity Bonus: Unique keywords = +5-10%
- Consistency Bonus: Multiple keyword types = +5%

**Quality Factor** (multiplier 0.5-1.0)
- Reduces boost if OCR quality is poor
- Poor OCR (<30% confidence) = 0.5x multiplier
- Good OCR (>50% confidence) = 1.0x multiplier

**Example Calculation:**
```
Document: Italian ID back side
- MRZ pattern detected (35 '<' chars) = 88% confidence
- Adjusted to: BACK with 88.89% confidence
- Method: MRZ pattern (most reliable)
```

### MRZ Pattern Detection

Back-side documents have **Machine Readable Zone** - a specific strip with repeated `<` characters.

```
Example:
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
ROSSI<<MARIO<<<<<<<<<<<<<<<<<<<<<<<<<
```

**Detection Algorithm:**
- Count consecutive '<' characters
- If â‰¥5 characters found = Back side (very reliable)
- Takes priority over all keyword matching

### Intelligent Pairing

When one document is clearly identified and another is ambiguous:
```
Page 1: Document A = FRONT (highly confident)
Page 1: Document B = UNKNOWN (low confidence from OCR)
Result: Document B = BACK (via pairing heuristic)
Confidence: 65% (lower due to heuristic)
```

---

## ğŸ—ï¸ Technical Architecture

### Module Responsibilities

| Module | Purpose |
|--------|---------|
| **app.py** | User interface, session management, result display |
| **identity_detection.py** | Classification logic, keyword analysis, heuristic engine |
| **document_segmentation.py** | Document detection, segmentation, OCR coordination |
| **config_loader.py** | Configuration management, dynamic keyword loading |
| **visualization.py** | Bounding box drawing, image annotation |
| **document_processor.py** | PDF page extraction, image processing |
| **content_extraction.py** | OCR with multiple modes (fast/full), image resizing |
| **text_cleaner.py** | Text normalization, artifact removal (????, null bytes, etc.) |
| **clarity_check.py** | Ink ratio calculation for document clarity |
| **confidence_check.py** | OCR confidence calculation |

### Data Flow Diagram

```
Input: PDF file
  â†“
extract_page_data() - Page extraction at 150 DPI
  â†“
process_identity_documents() - Main pipeline
  â”œâ†’ process_page_with_multiple_documents()
  â”‚   â”œâ†’ segment_documents_on_page() - Find document boundaries
  â”‚   â”‚   â””â†’ extract_text_content() - OCR text
  â”‚   â”‚       â””â†’ clean_text() - Remove artifacts
  â”‚   â””â†’ classify_identity_document() - Classify + score
  â”‚       â”œâ†’ calculate_ink_ratio() - Clarity
  â”‚       â””â†’ calculate_ocr_confidence() - Quality
  â”œâ†’ _analyze_keyword_frequency() - Cross-doc analysis
  â”œâ†’ _apply_frequency_based_adjustment() - Boost confidence
  â””â†’ _apply_classification_heuristics() - Smart pairing
      â”œâ†’ MRZ detection
      â”œâ†’ Keyword matching
      â””â†’ Heuristic logic
  â†“
Output: Classified documents with confidence
```

### Text Cleaning Pipeline

```
Raw OCR text
  â†“
Remove null bytes (\x00)
  â†“
Remove replacement characters (????, ?, etc.)
  â†“
Normalize whitespace (multiple â†’ single)
  â†“
Clean empty lines
  â†“
Strip leading/trailing whitespace
  â†“
Final cleaned text
```

---

## ğŸ“Š Examples

### Example 1: Italian ID (Clear Case)

```
Input: Italian ID front page
Extracted: "CARTA D'IDENTITÃ€ NOME: ROSSI MARIO DATA DI NASCITA: 01/01/1990"

Analysis:
- Keywords: "carta d'identitÃ " (front), "nome" (front), "data di nascita" (front)
- MRZ: None
- Matched keywords: 3
- Frequency: 1 page with these keywords
- Quality: Good (92% OCR confidence)

Result:
- Type: National ID
- Side: FRONT
- Confidence: 100%
- Method: front_keywords
```

### Example 2: Italian ID (Back Side with MRZ)

```
Input: Italian ID back page
Extracted: "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< FIRMA________________ SCADENZA 2028"

Analysis:
- Keywords: "firma" (back), "scadenza" (back)
- MRZ: 35 '<' characters detected â†’ BACK
- Quality: Good OCR

Result:
- Type: National ID
- Side: BACK
- Confidence: 88.89%
- Method: mrz_pattern (highest priority)
```

### Example 3: Ambiguous Case (Poor OCR)

```
Input: ID back page with poor image quality
Extracted: "????? ???????? ?????? ???  [56 chars of garbage]"

Analysis:
- OCR quality: Only 56 chars extracted (poor)
- Keywords: Insufficient (mostly corrupted)
- Direct classification: No clear match
- Page context: FRONT detected on same page
- Heuristic: paired_front_back rule applies

Result:
- Type: National ID
- Side: BACK
- Confidence: 65%
- Method: paired_front_back (heuristic)
```

---

## ğŸ§¹ Code Quality

### Optimization Status

âœ… **Zero Unused Code**
- No dead functions or imports
- No code duplication
- Single source of truth for all configuration
- Production-ready optimization status

### Development History

**Phase 1: Multi-Document Detection** âœ“
- Implemented segmentation for 2+ documents per page
- Added intelligent front/back pairing
- Eliminated overlapping bounding boxes (0px gap)

**Phase 2: UI & Text Cleanup** âœ“
- Created text_cleaner.py module
- Added emoji indicators and color coding
- Improved visual presentation with bordered cards

**Phase 3: Threshold Management** âœ“
- Set default confidence threshold to 70%
- Updated "National ID Present" logic to check against threshold
- Shows âœ… Yes only when confidence >= threshold

**Phase 4: Code Optimization (Multi-Round)** âœ“
- Round 1: Removed 9 unused files
- Round 2: Removed 5 unused imports and 2 functions
- Round 3: Consolidated duplicate code (single source of truth)
- Result: Zero dead code remaining

---

## ğŸ› Troubleshooting

### Tesseract Not Found
```bash
# Windows: Verify installation path
C:\Program Files\Tesseract-OCR\tesseract.exe

# Set manually if needed:
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
```

### Low Confidence/No Detection
1. Check PDF quality (should have contrast)
2. Review detected keywords in Advanced Analysis
3. Adjust thresholds in sidebar
4. Check config.json keywords for your language

### Multiple Documents Not Detected
1. Ensure documents are clearly separated spatially
2. Check emptiness threshold isn't too high
3. Review segmentation in visualization
4. Verify document contrast

### Missing Text In Bounding Boxes
- Text cleaning removes OCR artifacts like '????' and control characters
- This is intentional behavior to improve readability
- Original text available in Advanced Analysis section

---

## ğŸ“ Support

For issues or feature requests, review:
- Configuration in config.json
- Advanced Analysis section in UI for detailed detection info
- Technical Architecture section above for algorithm details

---

**Version**: 1.0  
**Status**: Production-Ready  
**Code Quality**: Fully Optimized (Zero Dead Code)  
**Last Updated**: February 17, 2026
